{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bdc1896",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center\"> Escuela Técnica Superior de Ingeniería Informática</p>\n",
    "<p style=\"text-align: center\">Universidad de Sevilla</p>\n",
    "<p style=\"text-align: center\">Máster en Lógica, Ciencias de la Computación e Inteligencia Artificial </p>\n",
    "<p style=\"text-align: center\">Procesamiento del Lenguaje Natural </p>\n",
    "<p style=\"text-align: center\">Evaluación Continua: Identificación del lenguaje y limpieza de datasets </p>\n",
    "<p> </p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4451d6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import random\n",
    "import itertools\n",
    "import re\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "import pandas as pd\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0330ca",
   "metadata": {},
   "source": [
    "# Preparación del dataset de trabajo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abacbeb4",
   "metadata": {},
   "source": [
    "## Tarea 1: Construcción de ficheros con los corpus\n",
    "\n",
    "La primera tarea que se be realizar una vez descargados los ficheros es construir los \n",
    "datasets de entrenamiento y evaluación, y además hacerlo de una forma balanceada.\n",
    "Para ello, se han de generar ficheros de entrenamiento para cada uno de los lenguajes \n",
    "elegidos formados por 100.000 líneas, y un fichero de evaluación formado por 10.000 \n",
    "líneas. Evidentemente el contenido del fichero de evaluación debe ser diferente al de \n",
    "entrenamiento. Una estrategia adecuada es utilizar un modelo de selección aleatoria de \n",
    "forma que al recorrer el fichero original, de cada 11 líneas recorridas, 10 (seleccionadas \n",
    "aleatoriamente) sean enviadas al fichero de entrenamiento y 1 al de evaluación.\n",
    "\n",
    "Para poder acceder a los directorios de forma muy fácil, se va a emplear la librería ```Pathlib```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a6d39fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL que contiene todos los elementos (MODIFICAR CON EL DIRECTORIO DONDE ESTÉN LOS FICHEROS EN CRUDO)\n",
    "directory = Path(r\"C:\\Users\\PC\\Documents\\Master_MULCIA\\2ºCuatrimestre\\PLN\\data\")\n",
    "\n",
    "for file in directory.glob(\"europarl-v7*\"):\n",
    "    with open(file, 'r',encoding='utf-8') as file_org, \\\n",
    "     open(f'{directory}/{file.name[-2:]}_train.txt', 'w',encoding='utf-8') as file_train, \\\n",
    "     open(f'{directory}/{file.name[-2:]}_test.txt', 'w',encoding='utf-8') as file_test:\n",
    "        cnt_test,cnt_train=0,0\n",
    "        for i, line in enumerate(file_org):\n",
    "            if random.randint(1, 11) == 1 and cnt_test < 10000:\n",
    "                file_test.write(line)\n",
    "                cnt_test+=1\n",
    "            elif random.randint(1, 11) != 1 and cnt_train <100000:\n",
    "                file_train.write(line)\n",
    "                cnt_train+=1\n",
    "            if cnt_test+cnt_train == 110000:\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2327552",
   "metadata": {},
   "source": [
    "## Tarea 2: \n",
    "\n",
    "La segunda tarea de este apartado consiste en normalizar los corpus de evaluación y \n",
    "entrenamiento. Está claro que para esta tarea los signos de puntuación no aportan ninguna \n",
    "información. Por tanto, se deben normalizar los textos de forma que se eliminen los \n",
    "signos de puntuación, así como las expresiones numéricas. A la hora de realizar esta \n",
    "operación hay que tener cuidado con signos que sí pueden ser relevantes como puede ser \n",
    "el apóstrofe en los lenguajes donde se aplique. Por tanto, como parte de esta tarea se ha \n",
    "de indicar qué signos se eliminan, y cuáles no, justificando la decisión. Así mismo, podría \n",
    "ser interesante normalizar el texto en cuanto a mayúsculas. No obstante, este criterio se \n",
    "deja a decisión de cada alumno, debiendo justificar por qué toma una u otra decisión.\n",
    "\n",
    "\n",
    "Como cada idioma tiene sus características y peculiaridades, por lo que la limpieza de los textos debe ser adaptada a cada idioma. Por ejemplo, en español y francés, es importante no borrar los acentos ortográficos, mientras que en inglés no tiene.\n",
    "\n",
    "Así que se va a generar unas funciones para poder limpiar cada corpus de cada idioma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaf10b2",
   "metadata": {},
   "source": [
    "En primer lugar, hemos generado una función para eliminar las características irrelevantes de todos los idiomas. Como los signos de puntuación, paréntesis, y caracteres numéricos. También, se ha decidido normalizar los textos en cuanto a las mayúsculas, esto es debido a que nos puede traer beneficios como reducir la dispersión de los datos, mejorar la consistencia y simplifica el preprocesado.\n",
    "\n",
    "Para la justificación de lo que se ha borrado en los idiomas se ha empleado la [página de Babel](https://www.babbel.com/en/magazine/punctuation-marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4a5bae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_txt(txt:str) -> str:\n",
    "    txt = txt.lower()\n",
    "    txt = txt.replace(\"/\", \"\")\n",
    "    txt = re.sub(r'[.,;:!?()\\[\\]{}—\"\\']|-]', '', txt)\n",
    "    txt = re.sub(r'\\d+', '', txt)\n",
    "    txt = re.sub(r'\\s+', ' ', txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be22560",
   "metadata": {},
   "source": [
    "En español, eliminamos los signos de puntuación como la apertura de interrogación y exclamación, que son relevantes para el español,\n",
    "pero no para el resto de lenguajes. Además, las comillas también se pueden quitar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "080d5765",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_es = set(stopwords.words('spanish'))\n",
    "def clean_txt_es(txt:str) -> str:\n",
    "    txt = clean_txt(txt)\n",
    "    txt = re.sub(r'[¿¡]', '', txt)\n",
    "    txt = txt.replace(\"«\", \"\")\n",
    "    txt = txt.replace(\"»\", \"\")\n",
    "    txt = re.sub(r\"['\\\"]\", \"\", txt)\n",
    "    txt = txt.replace('-', '')\n",
    "    words = txt.split()\n",
    "    words_clean = [word for word in words if word.lower() not in stop_words_es]\n",
    "    txt_clean = ' '.join(words_clean)\n",
    "    return txt_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c2b315",
   "metadata": {},
   "source": [
    "En francés se pueden eliminar las mismas comillas que usamos los españoles, al igual que el italiano, debido a que vienen de la misma familia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9759733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_fr = set(stopwords.words('french'))\n",
    "def clean_txt_fr(txt):\n",
    "    txt = clean_txt(txt)\n",
    "    txt = txt.replace(\"«\", \"\")\n",
    "    txt = txt.replace(\"»\", \"\")\n",
    "    txt = txt.replace('-', '')\n",
    "    words = txt.split()\n",
    "    words_clean = [word for word in words if word.lower() not in stop_words_fr]\n",
    "    txt_clean = ' '.join(words_clean)\n",
    "    return txt_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b0cb6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_it = set(stopwords.words('italian'))\n",
    "def clean_txt_it(txt):\n",
    "    txt = clean_txt(txt)\n",
    "    txt = txt.replace(\"«\", \"\")\n",
    "    txt = txt.replace(\"»\", \"\")\n",
    "    txt = txt.replace('-', '')\n",
    "    words = txt.split()\n",
    "    words_clean = [word for word in words if word.lower() not in stop_words_it]\n",
    "    txt_clean = ' '.join(words_clean)\n",
    "    return txt_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1df0aa",
   "metadata": {},
   "source": [
    "En inglés y alemán se parecen mucho, debido a que vienen de la misma familia, por lo que podemos eliminar las comillas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "305dd67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_en = set(stopwords.words('english'))\n",
    "\n",
    "def clean_txt_en(txt):\n",
    "    txt = clean_txt(txt)\n",
    "#     txt = re.sub(r'[„“]', '', txt)\n",
    "    txt = re.sub(r\"[‘’'“”\\\"”]+\", \"\", txt)\n",
    "    txt = txt.replace('-', '')\n",
    "    words = txt.split()\n",
    "    words_clean = [word for word in words if word.lower() not in stop_words_en]\n",
    "    txt_clean = ' '.join(words_clean)\n",
    "    \n",
    "    return txt_clean\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84586051",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_de = set(stopwords.words('german'))\n",
    "def clean_txt_de(txt):\n",
    "    txt = clean_txt(txt)\n",
    "    txt = re.sub(r\"[‘’'“”\\\"”]+\", \"\", txt)\n",
    "    txt = txt.replace('-', '')\n",
    "    words = txt.split()\n",
    "    words_clean = [word for word in words if word.lower() not in stop_words_de]\n",
    "    txt_clean = ' '.join(words_clean)\n",
    "    return txt_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c8ba7e",
   "metadata": {},
   "source": [
    "En polaco, NLTK [no tiene su propio conjunto de palabras stopwords](https://stackoverflow.com/questions/54573853/nltk-available-languages-for-stopwords). Por lo que se ha seleccionado un un fichero con [stopwords en polaco](https://github.com/bieli/stopwords/blob/master/polish.stopwords.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be2026fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabic', 'azerbaijani', 'basque', 'bengali', 'catalan', 'chinese', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hebrew', 'hinglish', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a60b55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_pl = []\n",
    "with open('polish.stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        stop_words_pl.append(line.strip())\n",
    "stop_words_pl = set(stop_words_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fa0fa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_txt_pl(txt):\n",
    "    txt = clean_txt(txt)\n",
    "    txt = txt.replace('-', '')\n",
    "    words = txt.split()\n",
    "    words_clean = [word for word in words if word.lower() not in stop_words_pl]\n",
    "    txt_clean = ' '.join(words_clean)\n",
    "    return txt_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd07f8d",
   "metadata": {},
   "source": [
    "Generamos un diccionario con las distintas funciones y no tener que usar condicionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b8242d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "funciones_limpieza = {\n",
    "    'es': clean_txt_es,\n",
    "    'fr': clean_txt_fr,\n",
    "    'it': clean_txt_it,\n",
    "    'de': clean_txt_de,\n",
    "    'en': clean_txt_en,\n",
    "    'pl': clean_txt_pl,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53273478",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in directory.glob('*_[train|test]*'):\n",
    "    with open(file, 'r',encoding='utf-8') as file_org:\n",
    "        language = file.name.split(\"_\")[0]\n",
    "        file_name = file.name.split(\".\")[0]\n",
    "        clean_text = \"\"\n",
    "        for line in file_org:\n",
    "            clean_line = funciones_limpieza[language](line)\n",
    "            clean_text += clean_line\n",
    "        with open(f'{directory}/{file.name.split(\".\")[0]}_clean.txt', 'w',encoding='utf-8') as file_dest:\n",
    "            file_dest.write(clean_text)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae41e98b",
   "metadata": {},
   "source": [
    "## Tarea 3: Obtener las  100 palabras más frecuentes del corpues de entrenamiento\n",
    "\n",
    "La tercera tarea de este apartado consistirá en la obtención, para cada uno de los 6 \n",
    "lenguajes elegidos, de las 100 palabras más frecuentes en el corpus de entrenamiento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "742f5329",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicc={}\n",
    "for file in directory.glob('*_test_clean*'):\n",
    "    with open(file, 'r', encoding='utf-8') as file_org:\n",
    "        text=file_org.read()\n",
    "        language = file.name.split(\"_\")[0]\n",
    "        words = nltk.tokenize.word_tokenize(text)\n",
    "        fdist = FreqDist(words)\n",
    "        top_words = [word for word, frequency in fdist.most_common(100)]\n",
    "        dicc[language]=top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36dab4c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>de</th>\n",
       "      <th>en</th>\n",
       "      <th>es</th>\n",
       "      <th>fr</th>\n",
       "      <th>it</th>\n",
       "      <th>pl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kommission</td>\n",
       "      <td>european</td>\n",
       "      <td>comisión</td>\n",
       "      <td>a</td>\n",
       "      <td>commissione</td>\n",
       "      <td>panie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>europäischen</td>\n",
       "      <td>commission</td>\n",
       "      <td>presidente</td>\n",
       "      <td>plus</td>\n",
       "      <td>presidente</td>\n",
       "      <td>przewodniczący</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>union</td>\n",
       "      <td>president</td>\n",
       "      <td>unión</td>\n",
       "      <td>commission</td>\n",
       "      <td>essere</td>\n",
       "      <td>komisji</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>präsident</td>\n",
       "      <td>also</td>\n",
       "      <td>europea</td>\n",
       "      <td>cette</td>\n",
       "      <td>europea</td>\n",
       "      <td>europejskiej</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>möchte</td>\n",
       "      <td>would</td>\n",
       "      <td>parlamento</td>\n",
       "      <td>président</td>\n",
       "      <td>stati</td>\n",
       "      <td>unii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>staaten</td>\n",
       "      <td>system</td>\n",
       "      <td>programa</td>\n",
       "      <td>grande</td>\n",
       "      <td>sociale</td>\n",
       "      <td>została</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>schutz</td>\n",
       "      <td>given</td>\n",
       "      <td>momento</td>\n",
       "      <td>certains</td>\n",
       "      <td>necessario</td>\n",
       "      <td>jedynie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ziel</td>\n",
       "      <td>information</td>\n",
       "      <td>sector</td>\n",
       "      <td>partie</td>\n",
       "      <td>programma</td>\n",
       "      <td>panu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>situation</td>\n",
       "      <td>much</td>\n",
       "      <td>asuntos</td>\n",
       "      <td>beaucoup</td>\n",
       "      <td>altro</td>\n",
       "      <td>naszej</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>bedeutung</td>\n",
       "      <td>able</td>\n",
       "      <td>relación</td>\n",
       "      <td>moins</td>\n",
       "      <td>posizione</td>\n",
       "      <td>kraju</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              de           en          es          fr           it  \\\n",
       "0     kommission     european    comisión           a  commissione   \n",
       "1   europäischen   commission  presidente        plus   presidente   \n",
       "2          union    president       unión  commission       essere   \n",
       "3      präsident         also     europea       cette      europea   \n",
       "4         möchte        would  parlamento   président        stati   \n",
       "..           ...          ...         ...         ...          ...   \n",
       "95       staaten       system    programa      grande      sociale   \n",
       "96        schutz        given     momento    certains   necessario   \n",
       "97          ziel  information      sector      partie    programma   \n",
       "98     situation         much     asuntos    beaucoup        altro   \n",
       "99     bedeutung         able    relación       moins    posizione   \n",
       "\n",
       "                pl  \n",
       "0            panie  \n",
       "1   przewodniczący  \n",
       "2          komisji  \n",
       "3     europejskiej  \n",
       "4             unii  \n",
       "..             ...  \n",
       "95         została  \n",
       "96         jedynie  \n",
       "97            panu  \n",
       "98          naszej  \n",
       "99           kraju  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pd.DataFrame(dicc)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffb457b",
   "metadata": {},
   "source": [
    "# Implementación de un modelo TF-IDF para detección del lenguaje"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0002dde9",
   "metadata": {},
   "source": [
    "#  Evaluación del modelo de detección de lenguajes mediante TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba9ef14",
   "metadata": {},
   "source": [
    "# Limpieza de los corpus de entrenamiento y evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b8adfa",
   "metadata": {},
   "source": [
    "# TF-IDF con el modelo de corpus limpio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d680ed0",
   "metadata": {},
   "source": [
    "# Detección de lenguajes usando n-gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab1efce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51966c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
